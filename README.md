# üöÑ Shinkansen Travel Experience Prediction ‚Äì Hackathon Project üöÑ

This project was developed as part of a **hackathon** to predict **passenger satisfaction** (`Overall_Experience`) on the Shinkansen Bullet Train using **machine learning**. The model analyzes survey responses and travel data to help optimize passenger experience.

---

## üèÜ **Hackathon Overview**
- **Hackathon Name:** Shinkansen Travel Experience  
- **Date:** Jan 17, 12:00 AM - Jan 19, 11:59 PM  
- **Team Members:** Robert Lupo 
- **Challenge Statement:** Build a predictive model to determine whether a passenger had a satisfactory experience based on their survey responses and travel history.

---

## üìä **Project Goal**
- **Objective:** Predict whether a passenger is **satisfied (`1`)** or **unsatisfied (`0`)** with their overall experience.  
- **Impact:** By identifying key factors influencing satisfaction, train operators can **improve services**, reduce negative feedback, and **increase customer retention**.  

---

## üìÅ **Data Description**
### **üìú Dataset Provided in Hackathon**
1. **Train Data** (Labeled):
   - `Traveldata_train.csv` ‚Äì Passenger demographics & travel details.
   - `Surveydata_train.csv` ‚Äì Customer satisfaction ratings (target: `Overall_Experience`).

2. **Test Data** (Unlabeled for submission):
   - `Traveldata_test.csv` ‚Äì Same structure as training travel data.
   - `Surveydata_test.csv` ‚Äì Same structure as training survey data.

### **üìù Key Features**
| Feature | Description |
|---------|------------|
| `ID` | Unique passenger identifier |
| `Gender` | Male / Female |
| `Age` | Age of the passenger (numeric/binned) |
| `Customer_Type` | Loyal or Disloyal Customer |
| `Travel_Class` | Business / Economy |
| `Travel_Distance` | Distance traveled (km/miles) |
| `DepartureDelay_in_Mins` | Delay at departure (minutes) |
| `ArrivalDelay_in_Mins` | Delay at arrival (minutes) |
| `Seat_Comfort`, `Cleanliness`, `Onboard_Wifi_Service`, etc. | Survey-based satisfaction ratings |
| **`Overall_Experience`** | üéØ **Target Variable** (0 = Not Satisfied, 1 = Satisfied) |

---

## Repository Structure
shinkansen-travel-experience/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ Traveldata_train.csv
‚îÇ   ‚îú‚îÄ‚îÄ Surveydata_train.csv
‚îÇ   ‚îú‚îÄ‚îÄ Traveldata_test.csv
‚îÇ   ‚îî‚îÄ‚îÄ Surveydata_test.csv
‚îú‚îÄ‚îÄ notebook/
‚îÇ   ‚îú‚îÄ‚îÄ shinkansen-travel-experience.ipynb       # Main Jupyter notebook for EDA & modeling
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ submission.csv           # Final prediction output for Kaggle
‚îú‚îÄ‚îÄ README.md                    # Project overview & instructions
‚îî‚îÄ‚îÄ LICENSE                       # Open-source license

## üöÄ Getting Started

Follow these steps to set up the project and run the model.

### **1Ô∏è‚É£ Clone the Repository**
```
git clone https://github.com/your-username/shinkansen-travel-experience.git
cd shinkansen-travel-experience
```
### **2Ô∏è‚É£ Install Dependencies
```
pip install pandas numpy scikit-learn xgboost lightgbm catboost seaborn matplotlib
```
### **3Ô∏è‚É£ Obtain the Data
- **Ensure your CSV files are in the data/ directory, or update the paths in the notebook if they are stored elsewhere.**
- **If using a Kaggle dataset, download and place it in data/.**
### **4Ô∏è‚É£ Run the Jupyter Notebook
```
jupyter notebook
```
- **Open notebook/shinkansen_experience.ipynb and execute all cells.**
- **Follow the notebook workflow for data preprocessing, model training, evaluation, and predictions.**
### **5Ô∏è‚É£ Generate Predictions
- **Run the final model on the test data.**
- **Save the predictions to a CSV file:**
```
submission.to_csv('submission.csv', index=False)
```
- **Upload submission.csv to Kaggle or the hackathon platform for evaluation.**

## üèó Modeling Approach

### **1Ô∏è‚É£ Data Preprocessing**
- **Handling Missing Values**:
  - Imputed missing numerical values using **mean**.
  - Categorical features were filled using **mode (most frequent value)**.
  - Certain missing values were replaced with "Unknown" where appropriate.
  
- **Feature Engineering**:
  - **Binning `Age`**: Converted into categorical bins (`25`, `35`, `45`, `60`, `80`).
  - **Ordinal Encoding**: Converted ratings like "Poor" ‚Üí `1`, "Excellent" ‚Üí `5`.
  - **One-Hot Encoding**: Applied to nominal categorical variables (`Gender`, `Travel_Class`, `Customer_Type`).
  - **Scaling**: Used `StandardScaler` (Z-score normalization) for numerical features.

- **Data Splitting**:
  - **Train-Test Split**: `80%` for training, `20%` for testing.
  - **Cross-Validation**: Used **Stratified K-Fold (3-fold CV)** to ensure balanced class distribution.

---

### **2Ô∏è‚É£ Machine Learning Models Tested**
- **Baseline Model**:
  - **Logistic Regression**: Simple linear model for benchmarking.

- **Tree-Based Models**:
  - **Random Forest**: Ensemble learning with multiple decision trees.
  - **XGBoost**: Gradient boosting with advanced regularization.
  - **LightGBM**: Faster, memory-efficient boosting algorithm.
  - **CatBoost**: Handles categorical features natively, reducing preprocessing effort.

---

### **3Ô∏è‚É£ Hyperparameter Tuning**
- Used **`RandomizedSearchCV`** for quick optimization across multiple models.
- Parameters tuned:
  - **LightGBM**: `num_leaves`, `n_estimators`, `learning_rate`.
  - **CatBoost**: `depth`, `learning_rate`, `iterations`.
  - **Random Forest**: `n_estimators`, `max_depth`, `min_samples_split`.

- Optimized models were retrained on the full dataset before final predictions.

---

### **4Ô∏è‚É£ Model Evaluation**
- **Primary Metric**:  
  - **Accuracy**: Proportion of correct predictions.

- **Secondary Metrics** (for class imbalance handling):
  - **Precision, Recall, F1-score** (via `classification_report`).
  - **Confusion Matrix**: Evaluated misclassification patterns.

- **Feature Importance**:
  - Extracted feature importances from tree-based models.
  - Visualized top contributing features affecting `Overall_Experience`.

---

## üìä Results & Key Findings

### **1Ô∏è‚É£ Best Performing Model**
- **üèÜ CatBoost** achieved the highest accuracy of **95.76%**.
- **LightGBM** performed similarly with **95.64%**.
- **Random Forest** and **XGBoost** also performed well but were slightly less accurate.

---

### **2Ô∏è‚É£ Key Factors Influencing Passenger Satisfaction**
Using feature importance analysis from tree-based models, the **top drivers** of satisfaction were:

#### **üîù Top 5 Most Important Features**
| Rank | Feature | Importance |
|------|---------|------------|
| 1Ô∏è‚É£  | **Seat Comfort** | üìà Strongest predictor of satisfaction |
| 2Ô∏è‚É£  | **Cleanliness** | ‚ú® Highly correlated with satisfaction |
| 3Ô∏è‚É£  | **Onboard Service** | üõé Passengers who rated onboard service poorly were more likely to be dissatisfied |
| 4Ô∏è‚É£  | **Arrival Time Convenience** | ‚è≥ Directly impacted overall experience |
| 5Ô∏è‚É£  | **Travel Class** | üè¢ Business class passengers had consistently higher satisfaction |

---

### **3Ô∏è‚É£ Interesting Insights from Data Analysis**
‚úÖ **Seat Comfort & Cleanliness** were **the biggest factors** affecting customer satisfaction.  
‚úÖ **Onboard Service & Online Support** also had a significant impact.  
‚úÖ **Business Class passengers had much higher satisfaction** than Economy travelers.  
‚úÖ **Longer travel distances slightly reduced satisfaction**, possibly due to fatigue.  
‚úÖ **Wi-Fi quality had a weaker impact** than expected, possibly because many travelers didn‚Äôt use it.

---

### **4Ô∏è‚É£ Confusion Matrix & Misclassifications**
- **Precision & Recall** for satisfied customers (`Overall_Experience = 1`) were both **above 94%**, meaning the model was highly effective at distinguishing between satisfied and unsatisfied passengers.
- **Common Misclassifications**:
  - Some **satisfied passengers were predicted as unsatisfied**, likely due to **delays** despite positive onboard ratings.
  - A few **unsatisfied passengers were predicted as satisfied**, possibly because they gave decent ratings in certain areas but had strong complaints in others.

---

### **5Ô∏è‚É£ Areas for Future Improvement**
üîπ **Further hyperparameter tuning** to improve model generalization.  
üîπ **Consider adding interaction terms** (e.g., combining "Seat Comfort" and "Travel Class").  
üîπ **Explore time-series factors**, such as peak travel seasons affecting satisfaction.  
üîπ **Investigate class imbalance strategies** (though the dataset appeared fairly balanced).  

## üìú License

This project is licensed under the **MIT License**.  
You are free to use, modify, and distribute this project as long as proper attribution is given.

See the [LICENSE](LICENSE) file for full details.

---

## üì© Contact & Acknowledgments

### **üë§ Author**
- **Robert Lupo**  
- üìß Email: treylupo1197@gmail.com  
- üîó LinkedIn: [[robertlupo1997]](https://www.linkedin.com/in/robertlupo1997/)  
- üèÜ GitHub: [[robertlupo1997]](https://github.com/robertlupo1997)  

### **üôè Acknowledgments**
- **Hackathon Organizers**: MIT Institute for Data, Systems, and Society (IDSS) for providing the challenge and dataset.  
- **Machine Learning Community**: Special thanks to Kaggle discussions, Stack Overflow, and open-source contributors for valuable insights on hyperparameter tuning, feature engineering, and modeling strategies.  
- **Python & ML Libraries**:  
  - **scikit-learn**: For model training and evaluation  
  - **XGBoost, LightGBM, CatBoost**: For advanced boosting techniques  
  - **Pandas & NumPy**: For efficient data processing  
  - **Seaborn & Matplotlib**: For visualizing insights  

### **üí° Want to Collaborate?**
üöÄ If you're interested in working together or have feedback, feel free to reach out!  
Contributions, suggestions, and PRs are **always welcome**!  

> ‚≠ê If you found this project helpful, give it a **star** on GitHub!
